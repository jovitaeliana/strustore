{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84a69de",
   "metadata": {},
   "source": [
    "## Create Item Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafac727",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMS_LIST = \"items.json\"\n",
    "ITEMS_URL = \"items-url.csv\"\n",
    "MENU = \"./menu/\"\n",
    "LISTING_JSON = \"./auctions/json\"\n",
    "LISTING_IMG = \"./auctions/raw\"\n",
    "SCRAPE_LISTING_LIMIT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import math \n",
    "\n",
    "items_file = ITEMS_LIST\n",
    "url_file = ITEMS_URL\n",
    "listings_per_page = 20\n",
    "\n",
    "# Opening JSON file return as dict\n",
    "f = open(items_file)\n",
    "data = json.load(f)\n",
    "\n",
    "with open(url_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    # File Header\n",
    "    f.write(\"id,url\\n\")\n",
    "    for d in data['DeviceFamilies']:\n",
    "        item_id = d['id']\n",
    "        code = d['code'] or \"\"\n",
    "        search = d['search'] or \"\"\n",
    "        children = d['children']\n",
    "        #pull 20 listings each subtype if Family has a category\n",
    "        if code or search:\n",
    "            for page in range(1,(len(children)*2)+1):\n",
    "                url = (f\"https://zenmarket.jp/en/yahoo.aspx?\"f\"c={code}&q={search}&p={page}\")\n",
    "                f.write(f\"{item_id},{url}\\n\")\n",
    "                print(url)  # optional: show on console\n",
    "        else:\n",
    "            for c in children:\n",
    "                if not c['ignore']:\n",
    "                    code = c['code']\n",
    "                    search = c['search'] or \"\"\n",
    "                    url_1 = (f\"https://zenmarket.jp/en/yahoo.aspx?\"f\"c={code}&q={search}&p=1\")\n",
    "                    url_2 = (f\"https://zenmarket.jp/en/yahoo.aspx?\"f\"c={code}&q={search}&p=2\")\n",
    "                    f.write(f\"{item_id},{url_1}\\n\")\n",
    "                    f.write(f\"{item_id},{url_2}\\n\")\n",
    "                    print(url_1)\n",
    "                    print(url_2)\n",
    "\n",
    "# Closing file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfe10",
   "metadata": {},
   "source": [
    "## Scrapy: Retrieve html from each URL (5sec delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7c10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Path to your global vals\n",
    "items_file = ITEMS_URL\n",
    "menu = MENU\n",
    "\n",
    "# Ensure the base folder exists\n",
    "os.makedirs(items_listing_menu, exist_ok=True)\n",
    "\n",
    "with open(items_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        item_id = row['id']\n",
    "        url = row['url']\n",
    "\n",
    "        print(f\"Fetching {url}\")\n",
    "\n",
    "        # Making the Request\n",
    "        try:\n",
    "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create a folder per item_id\n",
    "        item_folder = os.path.join(items_listing_menu, item_id)\n",
    "        os.makedirs(item_folder, exist_ok=True)\n",
    "\n",
    "        # Determine the next file number\n",
    "        existing_files = [\n",
    "            fname for fname in os.listdir(item_folder)\n",
    "            if fname.endswith('.html') and fname[:-5].isdigit()\n",
    "        ]\n",
    "        next_number = (\n",
    "            max([int(fname[:-5]) for fname in existing_files], default=0) + 1\n",
    "        )\n",
    "\n",
    "        # Save the HTML\n",
    "        filename = os.path.join(item_folder, f\"{next_number}.html\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        print(f\"Saved response to {filename}\")\n",
    "\n",
    "        # Wait 5 seconds before next request\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"All listing menus retrieved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9709c50",
   "metadata": {},
   "source": [
    "## Read HTML and gather data from listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de25eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "\n",
    "# Inputs\n",
    "items_listing_menu = MENU\n",
    "items_listing_json = LISTING_JSON\n",
    "items_listing_img = LISTING_IMG\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(items_listing_json, exist_ok=True)\n",
    "os.makedirs(items_listing_img, exist_ok=True)\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Find all HTML files recursively\n",
    "html_files = glob.glob(os.path.join(items_listing_menu, \"**/*.html\"), recursive=True)\n",
    "\n",
    "# Group files by item_id\n",
    "files_by_item = defaultdict(list)\n",
    "for html_path in html_files:\n",
    "    item_id = os.path.basename(os.path.dirname(html_path))\n",
    "    files_by_item[item_id].append(html_path)\n",
    "\n",
    "if not files_by_item:\n",
    "    print(\"No HTML files found.\")\n",
    "else:\n",
    "    for item_id, file_paths in files_by_item.items():\n",
    "        listings = []\n",
    "\n",
    "        for html_path in sorted(file_paths):\n",
    "            with open(html_path, encoding=\"utf-8\") as f:\n",
    "                html = f.read()\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            parent_div = soup.find(\"div\", id=\"yahoo-search-results\")\n",
    "            if not parent_div:\n",
    "                print(f\"No 'yahoo-search-results' div found in {html_path}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            divs = parent_div.find_all(\"div\", class_=\"yahoo-search-result\")\n",
    "            # No slicing hereâ€”keep them all\n",
    "\n",
    "            for div in divs:\n",
    "                # Get TITLE\n",
    "                a_tag = div.find(\"a\", class_=\"auction-url\")\n",
    "                if not a_tag:\n",
    "                    print(\"No <a> tag found inside div, skipping this listing.\")\n",
    "                    continue\n",
    "                title = a_tag.get_text(strip=True)\n",
    "\n",
    "                # Translate TITLE JP > EN\n",
    "                auction_title_jp = title\n",
    "                auction_title_en = \"\"\n",
    "                if auction_title_jp:\n",
    "                    try:\n",
    "                        translated = translator.translate(auction_title_jp, src=\"ja\", dest=\"en\")\n",
    "                        auction_title_en = translated.text\n",
    "                    except Exception as e:\n",
    "                        print(f\"Translation error: {e}\")\n",
    "                        auction_title_en = \"\"\n",
    "\n",
    "                # Get IMAGE URL\n",
    "                img_tag = div.find(\"div\", class_=\"img-wrap\").find(\"img\")\n",
    "                img_src = img_tag[\"src\"].strip() if img_tag else \"\"\n",
    "                clean_img = img_src.split(\"?\", 1)[0] if img_src else \"\"\n",
    "\n",
    "                # Get PRICE SPAN\n",
    "                price_div = div.find(\"div\", class_=\"auction-price\")\n",
    "                amount_span = price_div.find(\"span\", class_=\"amount\") if price_div else None\n",
    "\n",
    "                usd = amount_span.get(\"data-usd\", \"\").strip() if amount_span else \"\"\n",
    "                jpy = amount_span.get(\"data-jpy\", \"\").strip() if amount_span else \"\"\n",
    "                sgd = amount_span.get(\"data-sgd\", \"\").strip() if amount_span else \"\"\n",
    "                \n",
    "                # Get AUCTION ID\n",
    "                remove_watchlist_a = div.find(\"a\", class_=\"removeFromWatchList\")\n",
    "                auction_id = remove_watchlist_a.get(\"data-auctionid\", \"\").strip() if remove_watchlist_a else \"\"\n",
    "                \n",
    "                listings.append({\n",
    "                    \"auction_id\": auction_id,\n",
    "                    \"auction_title\": auction_title_jp,\n",
    "                    \"auction_title_en\": auction_title_en,\n",
    "                    \"auction_img\": clean_img,\n",
    "                    \"auction_price_usd\": usd,\n",
    "                    \"auction_price_jpy\": jpy,\n",
    "                    \"auction_price_sgd\": sgd\n",
    "                })\n",
    "\n",
    "        # Save all listings for this item_id to one JSON\n",
    "        output_path = os.path.join(items_listing_json, f\"{item_id}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            json.dump(listings, out_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(listings)} listings to {output_path}\")\n",
    "        \n",
    "        # Download images\n",
    "        item_img_folder = os.path.join(items_listing_img, item_id)\n",
    "        os.makedirs(item_img_folder, exist_ok=True)\n",
    "\n",
    "        for listing in listings:\n",
    "            img_url = listing[\"auction_img\"]\n",
    "            auction_id = listing[\"auction_id\"]\n",
    "            if not img_url or not auction_id:\n",
    "                print(\"Skipping image download for empty img or id.\")\n",
    "                continue\n",
    "\n",
    "            img_filename = os.path.join(item_img_folder, f\"{auction_id}.jpg\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(img_url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                with open(img_filename, \"wb\") as img_file:\n",
    "                    img_file.write(response.content)\n",
    "                # 1-second delay between images\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading image for {auction_id}: {e}\")\n",
    "\n",
    "        print(f\"Downloaded images for item {item_id}\")\n",
    "\n",
    "print(\"All processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scraper-venv)",
   "language": "python",
   "name": "scraper-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
